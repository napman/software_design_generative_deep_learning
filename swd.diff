diff --git a/keras_origin.py b/swd_modified.py
index f103360..7c470d3 100644
--- a/keras_origin.py
+++ b/swd_modified.py
@@ -12,52 +12,14 @@ Description: A simple DCGAN trained using `fit()` by overriding `train_step` on
 import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras import layers
-import numpy as np
-import matplotlib.pyplot as plt
-import os
-import gdown
-from zipfile import ZipFile
-
-"""
-## Prepare CelebA data
-We'll use face images from the CelebA dataset, resized to 64x64.
-"""
-
-os.makedirs("celeba_gan")
-
-url = "https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684"
-output = "celeba_gan/data.zip"
-gdown.download(url, output, quiet=True)
-
-with ZipFile("celeba_gan/data.zip", "r") as zipobj:
-    zipobj.extractall("celeba_gan")
-
-"""
-Create a dataset from our folder, and rescale the images to the [0-1] range:
-"""
 
+# データの前処理
 dataset = keras.preprocessing.image_dataset_from_directory(
-    "celeba_gan", label_mode=None, image_size=(64, 64), batch_size=32
+    "img_align_celeba", label_mode=None, image_size=(64, 64), batch_size=32
 )
 dataset = dataset.map(lambda x: x / 255.0)
 
-
-"""
-Let's display a sample image:
-"""
-
-
-for x in dataset:
-    plt.axis("off")
-    plt.imshow((x.numpy() * 255).astype("int32")[0])
-    break
-
-
-"""
-## Create the discriminator
-It maps a 64x64 image to a binary classification score.
-"""
-
+# 識別機(Discriminator)を作成
 discriminator = keras.Sequential(
     [
         keras.Input(shape=(64, 64, 3)),
@@ -70,18 +32,11 @@ discriminator = keras.Sequential(
         layers.Flatten(),
         layers.Dropout(0.2),
         layers.Dense(1, activation="sigmoid"),
-    ],
-    name="discriminator",
+    ]
 )
-discriminator.summary()
-
-"""
-## Create the generator
-It mirrors the discriminator, replacing `Conv2D` layers with `Conv2DTranspose` layers.
-"""
 
+# 生成器 (Generator) を作成
 latent_dim = 128
-
 generator = keras.Sequential(
     [
         keras.Input(shape=(latent_dim,)),
@@ -94,16 +49,10 @@ generator = keras.Sequential(
         layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding="same"),
         layers.LeakyReLU(alpha=0.2),
         layers.Conv2D(3, kernel_size=5, padding="same", activation="sigmoid"),
-    ],
-    name="generator",
+    ]
 )
-generator.summary()
-
-"""
-## Override `train_step`
-"""
-
 
+# GANクラスを作成し、train_stepメソッドをオーバーライド
 class GAN(keras.Model):
     def __init__(self, discriminator, generator, latent_dim):
         super(GAN, self).__init__()
@@ -124,24 +73,17 @@ class GAN(keras.Model):
         return [self.d_loss_metric, self.g_loss_metric]
 
     def train_step(self, real_images):
-        # Sample random points in the latent space
+        # 画像を生成し、本物の画像と組み合わせて訓練データを作成
         batch_size = tf.shape(real_images)[0]
         random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))
-
-        # Decode them to fake images
         generated_images = self.generator(random_latent_vectors)
-
-        # Combine them with real images
         combined_images = tf.concat([generated_images, real_images], axis=0)
-
-        # Assemble labels discriminating real from fake images
         labels = tf.concat(
             [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0
         )
-        # Add random noise to the labels - important trick!
-        labels += 0.05 * tf.random.uniform(tf.shape(labels))
+        labels += 0.05 * tf.random.uniform(tf.shape(labels)) # ランダムノイズを加える
 
-        # Train the discriminator
+        # 識別器の訓練
         with tf.GradientTape() as tape:
             predictions = self.discriminator(combined_images)
             d_loss = self.loss_fn(labels, predictions)
@@ -150,21 +92,18 @@ class GAN(keras.Model):
             zip(grads, self.discriminator.trainable_weights)
         )
 
-        # Sample random points in the latent space
+        # 画像生成のための準備
         random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))
-
-        # Assemble labels that say "all real images"
         misleading_labels = tf.zeros((batch_size, 1))
 
-        # Train the generator (note that we should *not* update the weights
-        # of the discriminator)!
+        # 生成器の訓練
         with tf.GradientTape() as tape:
             predictions = self.discriminator(self.generator(random_latent_vectors))
             g_loss = self.loss_fn(misleading_labels, predictions)
         grads = tape.gradient(g_loss, self.generator.trainable_weights)
         self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))
 
-        # Update metrics
+        # 訓練に関するメトリクスの更新
         self.d_loss_metric.update_state(d_loss)
         self.g_loss_metric.update_state(g_loss)
         return {
@@ -172,12 +111,7 @@ class GAN(keras.Model):
             "g_loss": self.g_loss_metric.result(),
         }
 
-
-"""
-## Create a callback that periodically saves generated images
-"""
-
-
+# 訓練プロセスのモニタリング設定
 class GANMonitor(keras.callbacks.Callback):
     def __init__(self, num_img=3, latent_dim=128):
         self.num_img = num_img
@@ -190,15 +124,9 @@ class GANMonitor(keras.callbacks.Callback):
         generated_images.numpy()
         for i in range(self.num_img):
             img = keras.preprocessing.image.array_to_img(generated_images[i])
-            img.save("generated_img_%03d_%d.png" % (epoch, i))
-
-
-"""
-## Train the end-to-end model
-"""
-
-epochs = 1  # In practice, use ~100 epochs
+            img.save("./generated_imgs_%03d_%03d.png" % (epoch, i))
 
+# GANのモデルを作成
 gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)
 gan.compile(
     d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),
@@ -206,12 +134,5 @@ gan.compile(
     loss_fn=keras.losses.BinaryCrossentropy(),
 )
 
-gan.fit(
-    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]
-)
-
-"""
-Some of the last generated images around epoch 30
-(results keep improving after that):
-![results](https://i.imgur.com/h5MtQZ7l.png)
-"""
\ No newline at end of file
+# 訓練プロセスの実行
+gan.fit(dataset, epochs=60, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)])
\ No newline at end of file
